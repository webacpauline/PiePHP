https://blog.cellenza.com/developpement-specifique/asp-net-mvc-vs-seo-part-2-sitemap-xml-et-robots-txt/


ASP.NET MVC VS SEO : PART 2 â€“ LE FICHIER ROBOTS.TXT
Posted by Georges Damien | Mai 27, 2014 | DÃ©veloppement spÃ©cifique, Web | 0  |     

Cette fois-ci, nous allons Ã©tudier une nouvelle notion utile au SEO Ã  savoir : lâ€™utilisation des fichiers â€œRobots.txtâ€ sur notre site MVC.

LE FICHIER ROBOT.TXT : KEZAKO ?

Câ€™est une ressource accessible directement via le web que lâ€™on retrouve sur la majoritÃ© des grands sites publics. Ils ne sont pas obligatoires et ne sont dâ€™ailleurs pas prÃ©sents lorsque vous initialisez un projet Web ASP.NET MVC par dÃ©faut dans Visual studio. Pour consulter le fichier â€œrobots.txtâ€ des sites internet (sâ€™ils en ont un), il vous suffit en gÃ©nÃ©ral dâ€™aller sur un site web et dâ€™ajouter Ã  lâ€™url â€œ/robots.txtâ€.

INTÃ‰RÃŠT POUR LE SEO
Comme son nom lâ€™indique, le fichier â€œrobots.txtâ€ sera exploitÃ© par les fameux robots de moteurs de recherche. Il est en gÃ©nÃ©ral Ã  la racine de votre site internet (cela dÃ©pend bien sur de votre implÃ©mentation) et surtout accessible via lâ€™url http://www.votre-domaine.fr/robots.txt. Son principal objectif est dâ€™empÃªcher lâ€™indexation par les moteurs de recherche de certaines pages de votre site : vous disposez ainsi dâ€™un instrument supplÃ©mentaire pour filtrer (et donc bloquer) des pages de votre site internet qui nâ€™ont pas vocation Ã  favoriser votre SEO. Le fichier robots.txt permet Ã©galement dâ€™indiquer aux robots des moteurs de recherche lâ€™emplacement du â€œsitemapâ€ de votre site (nous verrons le â€˜sitemapâ€™ dans mon prochain billet).

IMPLÃ‰MENTATION AVEC ASP.NET MVC DU ROBOTS.TXT
En gÃ©nÃ©ral, le fichier robots.txt est un fichier physique sur le serveur accessible en lecture seule. Il existe cependant plusieurs techniques pour mettre en place cette ressource sur un site ASP.NET MVC. Celle que je vais prÃ©senter ici passe par lâ€™implÃ©mentation via un HttpHandler : nous allons gÃ©nÃ©rer dynamiquement le flux au format â€œtexteâ€ en sortie. Cette technique permet dâ€™isoler en quelque sorte ce fichier du mÃ©canisme MVC et Razor : le controller MVC ne rend donc pas une â€œviewâ€ via son mÃ©canisme de routing, mais on rend simplement accessible en lecture une information via un canal annexe (le handler). Lâ€™intÃ©rÃªt de passer par cette technique est surtout de pouvoir gÃ©nÃ©rer dynamiquement des rÃ©fÃ©rences Ã  des pages de notre site au cas oÃ¹ lâ€™on voudrait automatiser sa gÃ©nÃ©ration : on Ã©vite ainsi la mise Ã  jour manuelle du fichier.

Pour cela, nous allons crÃ©er un dossier quâ€™on nommera â€œHelpersâ€ dans notre solution VS et crÃ©er une classe RobotsHandler.cs qui implÃ©mentera lâ€™interface IHttpHandler (oÃ¹ crÃ©er directement un ASP.NET Handler via lâ€™interface en faisant un click droit sur le dossier â€˜Helpersâ€™ > Add > new Item > ASP.NET Handler).

IMAGE(helperRobots)

Une fois lâ€™interface implÃ©mentÃ©e, nous allons implÃ©menter la gÃ©nÃ©ration du rendu du â€œrobots.txtâ€ via la mÃ©thode â€œProcessRequestâ€ :

IMAGE(processRequest)

Le Fichier Robots.txt devra contenir en premier lieu une ligne pour le â€œUser-agentâ€ : on y spÃ©cifiera les moteurs ciblÃ©s (tous les moteurs de recherche par exemple). Pour les restrictions des pages : on utilisera la clÃ© â€œDisallowâ€œ. Dans lâ€™exemple ci-dessus, on bloque le scan par les robots des pages â€œAboutâ€ et â€œContactâ€ de notre site : ces pages ne seront donc pas indexÃ©es (rÃ©fÃ©rencÃ©es). Pour bloquer lâ€™accÃ¨s de toutes les pages du site Ã  tous les robots. Par exemple, on mettra la ligne suivante :

contentBuilder.AppendLine(â€œDisallow: /â€);

Cela nâ€™empÃªche cependant pas lâ€™indexation du site par les moteurs de recherche : il faudra pour cela rajouter dans le header la meta suivante (comme vu dans le billet prÃ©cÃ©dent) :

<meta name=â€robotsâ€ content=â€noindexâ€ />

On peut Ã©galement spÃ©cifier le chemin du Sitemap via la clÃ© â€œSitemapâ€, comme le montre la figure au dessus.

MODIFICATION DU WEB.CONFIG
Il faut modifier notre web.config pour faire fonctionner notre Handler :

Dans la section <system.webServer><handlers> du  web.config, on rajoutera la ligne suivante pour enregistrer notre Handler :

<addname=â€Robotsâ€ verb=â€*â€ path=â€/robots.txtâ€ type=â€DemoRobotTXTsitemapXML.Helpers.RobotsHandlerâ€ preCondition=â€managedHandlerâ€œ/>

Dans le fichier RouteConfig (dossier App_Start), on va rajouter une ligne pour ignorer le mÃ©canisme de â€œrouting MVCâ€.

routes.IgnoreRoute(â€œrobots.txtâ€);

Une fois, notre â€œHandlerâ€ implÃ©mentÃ©, nous allons nous assurer que tout cela fonctionne en saisissant lâ€™url http://www.mon-site.fr/robots.txt . Le rÃ©sultat devrait ressembler Ã  la figure ci-dessous :

IMAGE(robotsResult)

Conclusion

Le fichier â€˜robots.txtâ€™ va permettre de mettre des restrictions aux robots dâ€™indexation des moteurs de recherche sur des pages de notre site internet. Ce qui va potentiellement amÃ©liorer notre rÃ©fÃ©rencement en fonction de notre politique SEO : on pourra ainsi mettre en avant des pages plutÃ´t que dâ€™autres. Dans mon prochain billet qui viendra complÃ©ter celui ci, Je parlerai du fichier SiteMap.xml qui justement nous permettra de mettre en avant les pages que nous souhaitons voir indexÃ©es. A trÃ¨s bientÃ´t ğŸ™‚

ex:

# Allow all robots to index this site.
user-agent: *
 
# Tell all robots not to index any of the pages under the /error path.
disallow: /error/
 
# Tell all robots to index the under the error/foo path.
allow: /error/foo/
 
# Add a link to the site-map. Unfortunately this must be an absolute URL.
sitemap: http://example.com/sitemap.xml